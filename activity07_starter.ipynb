{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungminchoi123/ECE539/blob/main/activity07_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vfwQ6YEhluba"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# makes printing more human-friendly\n",
        "np.set_printoptions(precision=3,suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qH1dZbHeDcb7"
      },
      "outputs": [],
      "source": [
        "# a) Load data\n",
        "\n",
        "colab = False  # Set to True if using colab\n",
        "if colab:\n",
        "    # May require changing paths to file\n",
        "    drive.mount('/content/drive')\n",
        "    with open('/content/drive/activity07_data.csv', 'r') as f:\n",
        "      data = np.genfromtxt(f,delimiter=',')\n",
        "else:\n",
        "    # May require changing paths to file\n",
        "    with open('/content/drive/activity07_data.csv', 'r', encoding='utf-8-sig') as f:\n",
        "      data = np.genfromtxt(f,delimiter=',')\n",
        "\n",
        "X = data[:,:-1]\n",
        "y = data[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ga08Oc2ZnwqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8754a96e-7b59-4676-e139-2410790ee53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of samples: 178\n",
            "num of feature dimensions: 13\n",
            "num of classes: 3\n",
            "class 1.0 has 59 samples\n",
            "class 2.0 has 71 samples\n",
            "class 3.0 has 48 samples\n"
          ]
        }
      ],
      "source": [
        "# b) number of samples, features dimension, the number of classes\n",
        "\n",
        "num_samples = X.shape[0]\n",
        "num_feats = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "unique_classes, counts = np.unique(y, return_counts=True)\n",
        "num_samples_per_class = dict(zip(unique_classes, counts))\n",
        "\n",
        "print(f'num of samples: {num_samples}')\n",
        "print(f'num of feature dimensions: {num_feats}')\n",
        "print(f'num of classes: {num_classes}')\n",
        "for cls in num_samples_per_class:\n",
        "  print(f'class {cls} has {num_samples_per_class[cls]} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NZpvsgo2oXIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b1542d-2602-4dbe-b275-42ab25d38b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of NaN before imputation: 5\n",
            "Total of NaN after imputation: 0\n"
          ]
        }
      ],
      "source": [
        "# c) check nan, data imputation\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "if np.sum(np.isnan(X)):\n",
        "  print('Total of NaN before imputation:', np.sum(np.isnan(X)))\n",
        "  X = KNNImputer(n_neighbors=2, weights=\"uniform\").fit_transform(X)\n",
        "  print('Total of NaN after imputation:', np.sum(np.isnan(X)))\n",
        "else:\n",
        "  print('no NaN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9DNf_PtDcb9"
      },
      "source": [
        "#### Q) How are the missing values completed when using\n",
        "\n",
        "---\n",
        "\n",
        "KNNImputer?\n",
        "A) Answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6Ue36ENusGef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d571d132-2772-475a-9ef5-80d29b74e2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size:  142\n",
            "testing data size:  36\n"
          ]
        }
      ],
      "source": [
        "# d) partition 80/20\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('training data size: ', X_train.shape[0])\n",
        "print('testing data size: ', X_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TnkQ2QfWvO44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de18cb84-285a-4824-ed30-4fe3e95dfa61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min training data in each dimension, after standardization: [-5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5.]\n",
            "max training data in each dimension, after standardization: [5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "min testing data in each dimension, after standardization: [-4.    -5.305 -3.182 -2.732 -4.13  -3.724 -4.662 -4.245 -5.032 -5.409\n",
            " -4.268 -4.89  -4.63 ]\n",
            "max testing data in each dimension, after standardization: [3.605 2.556 3.021 2.423 0.761 3.    2.574 4.434 1.551 2.247 1.748 4.341\n",
            " 6.048]\n"
          ]
        }
      ],
      "source": [
        "# e) standardize to -5 to 5\n",
        "\n",
        "X_train_min = np.min(X_train, axis=0)\n",
        "X_train_max = np.max(X_train, axis=0)\n",
        "X_train_standardized = 10 * (X_train - X_train_min) / (X_train_max - X_train_min) - 5\n",
        "print('min training data in each dimension, after standardization:', np.min(X_train_standardized, axis=0))\n",
        "print('max training data in each dimension, after standardization:', np.max(X_train_standardized, axis=0))\n",
        "\n",
        "# Warning: When standardizing the test set, we should use statistics like min or max computed from the training set.\n",
        "X_test_standardized = 10 * (X_test - X_train_min) / (X_train_max - X_train_min) - 5\n",
        "print('min testing data in each dimension, after standardization:', np.min(X_test_standardized, axis=0))\n",
        "print('max testing data in each dimension, after standardization:', np.max(X_test_standardized, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb9TfUC_Dcb_",
        "outputId": "78c60820-252b-48cd-88e2-f1b9486fc2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean training data in each dimension, after standardization: [ 0. -0.  0.  0. -0.  0.  0. -0.  0.  0.  0.  0. -0.]\n",
            "std training data in each dimension, after standardization: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "mean testing data in each dimension, after standardization: [ 0.13  -0.161  0.101  0.033 -0.238  0.047  0.127 -0.232 -0.153  0.001\n",
            "  0.023  0.13   0.197]\n",
            "std testing data in each dimension, after standardization: [0.945 0.87  0.91  0.825 0.846 0.907 0.977 0.823 0.9   0.977 0.881 0.91\n",
            " 1.182]\n"
          ]
        }
      ],
      "source": [
        "# f) standardize to 0 mean, unit variance\n",
        "\n",
        "X_train_mean = np.mean(X_train, axis=0)\n",
        "X_train_std = np.std(X_train, axis=0)\n",
        "X_train_standardized = (X_train - X_train_mean) / X_train_std\n",
        "print('mean training data in each dimension, after standardization:', np.mean(X_train_standardized, axis=0))\n",
        "print('std training data in each dimension, after standardization:', np.std(X_train_standardized, axis=0))\n",
        "\n",
        "# Warning: When standardizing the test set, we should use statistics like min or max computed from the training set.\n",
        "X_test_standardized = (X_test - X_train_mean) / X_train_std\n",
        "print('mean testing data in each dimension, after standardization:', np.mean(X_test_standardized, axis=0))\n",
        "print('std testing data in each dimension, after standardization:', np.std(X_test_standardized, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MhMyvX7TxO5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6b7afc-09bc-4df7-e894-41619decd006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training class distribution:  (array([2., 3.]), array([70, 48]))\n",
            "validation class distribution:  (array([1., 2.]), array([59,  1]))\n",
            "training class distribution:  (array([1., 2., 3.]), array([59, 12, 48]))\n",
            "validation class distribution:  (array([2.]), array([59]))\n",
            "training class distribution:  (array([1., 2.]), array([59, 60]))\n",
            "validation class distribution:  (array([2., 3.]), array([11, 48]))\n"
          ]
        }
      ],
      "source": [
        "# g) k fold\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "# Count the class distributions in each partition\n",
        "for train_index, val_index in kf.split(X, y):\n",
        "  print('training class distribution: ', np.unique(y[train_index], return_counts=True))\n",
        "  print('validation class distribution: ', np.unique(y[val_index], return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EqtiN5sX2ER7"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}